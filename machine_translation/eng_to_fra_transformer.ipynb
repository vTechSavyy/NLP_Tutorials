{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n"
     ]
    }
   ],
   "source": [
    "# Load in the data using the data loader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "input_lang, output_lang, dataloader, pairs = get_dataloader(BATCH_SIZE, 'eng', 'fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/savio/Documents/Tutorials/NLP_Tutorials/machine_translation/eng_to_fra_transformer.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/savio/Documents/Tutorials/NLP_Tutorials/machine_translation/eng_to_fra_transformer.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mSeq2SeqTransformer\u001b[39;00m \u001b[39mimport\u001b[39;00m Seq2SeqTransformer\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/savio/Documents/Tutorials/NLP_Tutorials/machine_translation/eng_to_fra_transformer.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Setup the transformer model:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/savio/Documents/Tutorials/NLP_Tutorials/machine_translation/eng_to_fra_transformer.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Tutorials/NLP_Tutorials/machine_translation/Seq2SeqTransformer.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPositionalEncoding\u001b[39;00m \u001b[39mimport\u001b[39;00m PositionalEncoding\n\u001b[1;32m      8\u001b[0m \u001b[39m# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Move to a separate file later\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTokenEmbedding\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n",
      "File \u001b[0;32m~/Documents/Tutorials/NLP_Tutorials/machine_translation/PositionalEncoding.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPositionalEncoding\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      7\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m      8\u001b[0m                  emb_size: \u001b[39mint\u001b[39m,\n\u001b[1;32m      9\u001b[0m                  dropout: \u001b[39mfloat\u001b[39m,\n\u001b[1;32m     10\u001b[0m                  maxlen: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m5000\u001b[39m):\n\u001b[1;32m     11\u001b[0m         \u001b[39msuper\u001b[39m(PositionalEncoding, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Tutorials/NLP_Tutorials/machine_translation/PositionalEncoding.py:22\u001b[0m, in \u001b[0;36mPositionalEncoding\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(dropout)\n\u001b[1;32m     20\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m'\u001b[39m\u001b[39mpos_embedding\u001b[39m\u001b[39m'\u001b[39m, pos_embedding)\n\u001b[0;32m---> 22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, token_embedding: Tensor):\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(token_embedding \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embedding[:token_embedding\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), :])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tensor' is not defined"
     ]
    }
   ],
   "source": [
    "from Seq2SeqTransformer import Seq2SeqTransformer\n",
    "import torch.nn as nn\n",
    "\n",
    "# Setup the transformer model:\n",
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = input_lang.n_words\n",
    "TGT_VOCAB_SIZE = output_lang.n_words\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 64\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer_model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer_model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer_model = transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
